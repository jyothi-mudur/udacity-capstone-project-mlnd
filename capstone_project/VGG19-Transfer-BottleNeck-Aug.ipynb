{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "\n",
    "## Capstone Project - Smile Detector\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In this project, CNNs are used to build models to detect if the person in the image is smiling or not.\n",
    "CelibA dataset is used for this purpose - http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
    "The data for this project is downloaded from Kaggle - https://www.kaggle.com/jessicali9530/celeba-dataset\n",
    "\n",
    "Context (from Kaggle)\n",
    "A popular component of computer vision and deep learning revolves around identifying faces for various applications from logging into your phone with your face or searching through surveillance images for a particular suspect. This dataset is great for training and testing models for face detection, particularly for recognising facial attributes such as finding people with brown hair, are smiling, or wearing glasses. Images cover large pose variations, background clutter, diverse people, supported by a large quantity of images and rich annotations. This data was originally collected by researchers at MMLAB, The Chinese University of Hong Kong.\n",
    "\n",
    "Content\n",
    "Overall\n",
    "\n",
    "202,599 number of face images of various celebrities\n",
    "10,177 unique identities, but names of identities are not given\n",
    "40 binary attribute annotations per image\n",
    "5 landmark locations\n",
    "Data Files\n",
    "\n",
    "img_align_celeba.zip: All the face images, cropped and aligned\n",
    "list_eval_partition.csv: Recommended partitioning of images into training, validation, testing sets. Images 1-162770 are training, 162771-182637 are validation, 182638-202599 are testing\n",
    "list_bbox_celeba.csv: Bounding box information for each image. \"x_1\" and \"y_1\" represent the upper left point coordinate of bounding box. \"width\" and \"height\" represent the width and height of bounding box\n",
    "list_landmarks_align_celeba.csv: Image landmarks and their respective coordinates. There are 5 landmarks: left eye, right eye, nose, left mouth, right mouth\n",
    "list_attr_celeba.csv: Attribute labels for each image. There are 40 attributes. \"1\" represents positive while \"-1\" represents negative.\n",
    "\n",
    "Since the training is done on a CPU, it is not practical to train on the entire dataset. Hence, a reasonable random subset of the dataset is considered in this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_H=218\n",
    "IMG_W=178\n",
    "IMG_D=3\n",
    "\n",
    "NUM_IMAGES = 10000\n",
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, we populate a few variables through the use of the load_files function from the scikit-learn library:\n",
    "\n",
    "train_files, valid_files, test_files - numpy arrays containing file paths to images\n",
    "train_targets, valid_targets, test_targets - numpy arrays containing onehot-encoded classification labels\n",
    "smile_names - list of string-valued smile categories for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 total smile categories.\n",
      "There are 15000 total smile images.\n",
      "\n",
      "There are 10000 training smile images.\n",
      "There are 2500 validation smile images.\n",
      "There are 2500 test smile images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path, load_content=False)\n",
    "    smile_files = np.array(data['filenames'])\n",
    "    smile_targets = np_utils.to_categorical(np.array(data['target']), 2)\n",
    "    return smile_files, smile_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('input/dataset/train')\n",
    "valid_files, valid_targets = load_dataset('input/dataset/validate')\n",
    "test_files, test_targets = load_dataset('input/dataset/test')\n",
    "\n",
    "smile_names = [item[:-1] for item in sorted(glob(\"input/dataset/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total smile categories.' % len(smile_names))\n",
    "print('There are %s total smile images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training smile images.' % len(train_files))\n",
    "print('There are %d validation smile images.' % len(valid_files))\n",
    "print('There are %d test smile images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using TensorFlow as backend, Keras CNNs require a 4D array as input, with shape\n",
    "\n",
    "(nb_samples,rows,columns,channels), \n",
    "where nb_samples corresponds to the total number of images (or samples), and rows, columns, and channels correspond to the number of rows, columns, and channels for each image, respectively.\n",
    "\n",
    "The path_to_tensor function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN. The function first loads the image and resizes it to an image that is  218x178  pixels. Next, the image is converted to an array, which is then resized to a 4D tensor. In this case, since we are working with color images, each image has three channels. Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "(1,218,178,3).\n",
    " \n",
    "The paths_to_tensor function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape\n",
    "\n",
    "(nb_samples,218,178,3).\n",
    " \n",
    "Here, nb_samples is the number of samples, or number of images, in the supplied array of image paths. Also as VGG19 pretrained model is used, the corresponding preprocess_input function is used for the necessary actions required for VGG19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(IMG_H, IMG_W))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (IMG_H, IMG_W, IMG_D)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, IMG_H, IMG_W, IMG_D) and return 4D tensor\n",
    "    x = preprocess_input(x)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "    \n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:04<00:00, 614.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "#train_tensors = paths_to_tensor(train_files)\n",
    "#valid_tensors = paths_to_tensor(valid_files)\n",
    "test_tensors = paths_to_tensor(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now consider the details of the pre-trained VGG19 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers: 22\n",
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 218, 178, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 218, 178, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 218, 178, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 109, 89, 64)       0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 109, 89, 128)      73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 109, 89, 128)      147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 54, 44, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 54, 44, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 54, 44, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 54, 44, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 54, 44, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 27, 22, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 27, 22, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 27, 22, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 27, 22, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 27, 22, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 13, 11, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 13, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 13, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 13, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 13, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 6, 5, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "orig_model = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                    input_shape=(IMG_H, IMG_W, IMG_D))\n",
    "print(\"number of layers:\", len(orig_model.layers))\n",
    "orig_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to extract the bottleneck features for the training and validation sets by running them on VGG-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "\n",
    "# dimensions of our images.\n",
    "\n",
    "\n",
    "top_model_weights_path = 'input/saved_models/bottleneck_fc_model.h5'\n",
    "train_data_dir = 'input/dataset/train'\n",
    "validation_data_dir = 'input/dataset/validate'\n",
    "nb_train_samples = 10000\n",
    "nb_validation_samples = 2500\n",
    "epochs = 50\n",
    "\n",
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(\n",
    "        preprocessing_function = preprocess_input)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG19(include_top=False, \n",
    "                               weights='imagenet', \n",
    "                               input_shape=(IMG_H, IMG_W, IMG_D))\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(IMG_H, IMG_W),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    \n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, nb_train_samples // BATCH_SIZE)\n",
    "    \n",
    "    np.save(open('vgg19_bottleneck_features/bottleneck_features_train.npy', 'wb'),\n",
    "            bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(IMG_H, IMG_W),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // BATCH_SIZE)\n",
    "    \n",
    "    np.save(open('vgg19_bottleneck_features/bottleneck_features_validation.npy', 'wb'),\n",
    "            bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n",
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train the 'top-model' for classification using MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=top_model_weights_path,\n",
    "                               verbose=1,save_best_only=True)\n",
    "\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_topmodel_vgg19_sigmoid.log')\n",
    "\n",
    "earlyStopping = EarlyStopping(verbose = 1, min_delta = 0.01, patience = 5)\n",
    "\n",
    "def train_top_model():\n",
    "    train_data = np.load(open('vgg19_bottleneck_features/bottleneck_features_train.npy', 'rb'))\n",
    "    train_labels = np.array(\n",
    "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "    train_labels = np_utils.to_categorical(train_labels, 2)\n",
    "    \n",
    "    print (\"Train data:\" , len(train_data) )\n",
    "    print (\"Train labels:\" , len(train_labels) )\n",
    "    \n",
    "    train_labels = train_labels[:len(train_data)]\n",
    "    print (\"Train labels:\" , len(train_labels) )\n",
    "    \n",
    "    \n",
    "    validation_data = np.load(open('vgg19_bottleneck_features/bottleneck_features_validation.npy','rb'))\n",
    "    validation_labels = np.array(\n",
    "        [0] * (nb_validation_samples // 2) + [ 1] * (nb_validation_samples // 2))\n",
    "    validation_labels = np_utils.to_categorical(validation_labels, 2)\n",
    "    \n",
    "    print (\"validation data:\" , len(validation_data) )\n",
    "    print (\"Validation  labels:\" , len(validation_labels) )\n",
    "    \n",
    "    validation_labels = validation_labels[:len(validation_data)]\n",
    "    print (\"Validation  labels (new):\" , len(validation_labels) )\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    #model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9),\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              validation_data=(validation_data, validation_labels),\n",
    "              callbacks = [checkpointer, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 9984\n",
      "Train labels: 10000\n",
      "Train labels: 9984\n",
      "validation data: 2496\n",
      "Validation  labels: 2500\n",
      "Validation  labels (new): 2496\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_24 (Flatten)         (None, 15360)             0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 256)               3932416   \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 3,932,930\n",
      "Trainable params: 3,932,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9984 samples, validate on 2496 samples\n",
      "Epoch 1/50\n",
      "9984/9984 [==============================] - 17s 2ms/step - loss: 1.0656 - accuracy: 0.7088 - val_loss: 0.5511 - val_accuracy: 0.7460\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55115, saving model to input/saved_models/bottleneck_fc_model.h5\n",
      "Epoch 2/50\n",
      "9984/9984 [==============================] - 17s 2ms/step - loss: 0.4785 - accuracy: 0.7816 - val_loss: 0.4812 - val_accuracy: 0.7961\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55115 to 0.48117, saving model to input/saved_models/bottleneck_fc_model.h5\n",
      "Epoch 3/50\n",
      "9984/9984 [==============================] - 16s 2ms/step - loss: 0.4181 - accuracy: 0.8118 - val_loss: 0.4667 - val_accuracy: 0.7957\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.48117 to 0.46666, saving model to input/saved_models/bottleneck_fc_model.h5\n",
      "Epoch 4/50\n",
      "9984/9984 [==============================] - 16s 2ms/step - loss: 0.3698 - accuracy: 0.8359 - val_loss: 0.4501 - val_accuracy: 0.8121\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.46666 to 0.45015, saving model to input/saved_models/bottleneck_fc_model.h5\n",
      "Epoch 5/50\n",
      "9984/9984 [==============================] - 17s 2ms/step - loss: 0.3422 - accuracy: 0.8470 - val_loss: 0.4524 - val_accuracy: 0.8081\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.45015\n",
      "Epoch 6/50\n",
      "9984/9984 [==============================] - 17s 2ms/step - loss: 0.3106 - accuracy: 0.8654 - val_loss: 0.4526 - val_accuracy: 0.8045\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.45015\n",
      "Epoch 7/50\n",
      "9984/9984 [==============================] - 17s 2ms/step - loss: 0.2955 - accuracy: 0.8735 - val_loss: 0.4567 - val_accuracy: 0.8137\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.45015\n",
      "Epoch 8/50\n",
      "9984/9984 [==============================] - 17s 2ms/step - loss: 0.2729 - accuracy: 0.8840 - val_loss: 0.4615 - val_accuracy: 0.8033\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.45015\n",
      "Epoch 9/50\n",
      "9984/9984 [==============================] - 16s 2ms/step - loss: 0.2545 - accuracy: 0.8905 - val_loss: 0.4493 - val_accuracy: 0.8105\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.45015 to 0.44930, saving model to input/saved_models/bottleneck_fc_model.h5\n",
      "Epoch 10/50\n",
      "9984/9984 [==============================] - 19s 2ms/step - loss: 0.2314 - accuracy: 0.9021 - val_loss: 0.4509 - val_accuracy: 0.8097\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.44930\n",
      "Epoch 11/50\n",
      "9984/9984 [==============================] - 21s 2ms/step - loss: 0.2213 - accuracy: 0.9070 - val_loss: 0.4609 - val_accuracy: 0.8141\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.44930\n",
      "Epoch 12/50\n",
      "9984/9984 [==============================] - 20s 2ms/step - loss: 0.2045 - accuracy: 0.9165 - val_loss: 0.4672 - val_accuracy: 0.8173\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.44930\n",
      "Epoch 13/50\n",
      "9984/9984 [==============================] - 21s 2ms/step - loss: 0.1922 - accuracy: 0.9237 - val_loss: 0.4609 - val_accuracy: 0.8185\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.44930\n",
      "Epoch 14/50\n",
      "9984/9984 [==============================] - 22s 2ms/step - loss: 0.1805 - accuracy: 0.9262 - val_loss: 0.4729 - val_accuracy: 0.8225\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.44930\n",
      "Epoch 15/50\n",
      "9984/9984 [==============================] - 20s 2ms/step - loss: 0.1704 - accuracy: 0.9302 - val_loss: 0.4865 - val_accuracy: 0.8217\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.44930\n",
      "Epoch 16/50\n",
      "9984/9984 [==============================] - 20s 2ms/step - loss: 0.1570 - accuracy: 0.9370 - val_loss: 0.4793 - val_accuracy: 0.8177\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.44930\n",
      "Epoch 17/50\n",
      "9984/9984 [==============================] - 21s 2ms/step - loss: 0.1481 - accuracy: 0.9433 - val_loss: 0.4938 - val_accuracy: 0.8241\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.44930\n",
      "Epoch 18/50\n",
      "9984/9984 [==============================] - 21s 2ms/step - loss: 0.1402 - accuracy: 0.9436 - val_loss: 0.5036 - val_accuracy: 0.8233\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.44930\n",
      "Epoch 19/50\n",
      "9984/9984 [==============================] - 21s 2ms/step - loss: 0.1343 - accuracy: 0.9470 - val_loss: 0.5058 - val_accuracy: 0.8165\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.44930\n",
      "Epoch 20/50\n",
      "9984/9984 [==============================] - 19s 2ms/step - loss: 0.1257 - accuracy: 0.9525 - val_loss: 0.5159 - val_accuracy: 0.8213\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.44930\n",
      "Epoch 21/50\n",
      "9984/9984 [==============================] - 20s 2ms/step - loss: 0.1218 - accuracy: 0.9532 - val_loss: 0.5171 - val_accuracy: 0.8193\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.44930\n",
      "Epoch 22/50\n",
      "9984/9984 [==============================] - 20s 2ms/step - loss: 0.1104 - accuracy: 0.9584 - val_loss: 0.5264 - val_accuracy: 0.8281\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.44930\n",
      "Epoch 23/50\n",
      "9984/9984 [==============================] - 21s 2ms/step - loss: 0.1062 - accuracy: 0.9598 - val_loss: 0.5211 - val_accuracy: 0.8205\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.44930\n",
      "Epoch 24/50\n",
      "9984/9984 [==============================] - 20s 2ms/step - loss: 0.1033 - accuracy: 0.9619 - val_loss: 0.5406 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.44930\n",
      "Epoch 25/50\n",
      "9984/9984 [==============================] - 20s 2ms/step - loss: 0.0953 - accuracy: 0.9654 - val_loss: 0.5549 - val_accuracy: 0.8245\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.44930\n",
      "Epoch 26/50\n",
      "9984/9984 [==============================] - 20s 2ms/step - loss: 0.0863 - accuracy: 0.9679 - val_loss: 0.5537 - val_accuracy: 0.8273\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.44930\n",
      "Epoch 27/50\n",
      "9984/9984 [==============================] - 20s 2ms/step - loss: 0.0850 - accuracy: 0.9680 - val_loss: 0.5556 - val_accuracy: 0.8237\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.44930\n",
      "Epoch 28/50\n",
      "9984/9984 [==============================] - 20s 2ms/step - loss: 0.0803 - accuracy: 0.9714 - val_loss: 0.5593 - val_accuracy: 0.8221\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.44930\n",
      "Epoch 29/50\n",
      "9984/9984 [==============================] - 19s 2ms/step - loss: 0.0785 - accuracy: 0.9716 - val_loss: 0.5809 - val_accuracy: 0.8229\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.44930\n",
      "Epoch 30/50\n",
      "9984/9984 [==============================] - 18s 2ms/step - loss: 0.0719 - accuracy: 0.9758 - val_loss: 0.5828 - val_accuracy: 0.8277\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.44930\n",
      "Epoch 31/50\n",
      "9984/9984 [==============================] - 19s 2ms/step - loss: 0.0679 - accuracy: 0.9759 - val_loss: 0.5564 - val_accuracy: 0.8269\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.44930\n",
      "Epoch 32/50\n",
      "9984/9984 [==============================] - 19s 2ms/step - loss: 0.0671 - accuracy: 0.9749 - val_loss: 0.5875 - val_accuracy: 0.8281\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.44930\n",
      "Epoch 33/50\n",
      "9984/9984 [==============================] - 19s 2ms/step - loss: 0.0601 - accuracy: 0.9807 - val_loss: 0.6096 - val_accuracy: 0.8281\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.44930\n",
      "Epoch 34/50\n",
      "9984/9984 [==============================] - 18s 2ms/step - loss: 0.0553 - accuracy: 0.9814 - val_loss: 0.6240 - val_accuracy: 0.8257\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.44930\n",
      "Epoch 35/50\n",
      "9984/9984 [==============================] - 16s 2ms/step - loss: 0.0581 - accuracy: 0.9808 - val_loss: 0.6050 - val_accuracy: 0.8217\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.44930\n",
      "Epoch 36/50\n",
      "9984/9984 [==============================] - 18s 2ms/step - loss: 0.0553 - accuracy: 0.9821 - val_loss: 0.6088 - val_accuracy: 0.8281\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.44930\n",
      "Epoch 37/50\n",
      "9984/9984 [==============================] - 19s 2ms/step - loss: 0.0517 - accuracy: 0.9820 - val_loss: 0.6208 - val_accuracy: 0.8253\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.44930\n",
      "Epoch 38/50\n",
      "9984/9984 [==============================] - 18s 2ms/step - loss: 0.0518 - accuracy: 0.9838 - val_loss: 0.6223 - val_accuracy: 0.8261\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.44930\n",
      "Epoch 39/50\n",
      "9984/9984 [==============================] - 16s 2ms/step - loss: 0.0494 - accuracy: 0.9835 - val_loss: 0.6371 - val_accuracy: 0.8257\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.44930\n",
      "Epoch 40/50\n",
      "9984/9984 [==============================] - 17s 2ms/step - loss: 0.0469 - accuracy: 0.9850 - val_loss: 0.6441 - val_accuracy: 0.8273\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.44930\n",
      "Epoch 41/50\n",
      "9984/9984 [==============================] - 20s 2ms/step - loss: 0.0468 - accuracy: 0.9838 - val_loss: 0.6524 - val_accuracy: 0.8261\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.44930\n",
      "Epoch 42/50\n",
      "9984/9984 [==============================] - 17s 2ms/step - loss: 0.0400 - accuracy: 0.9874 - val_loss: 0.6646 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.44930\n",
      "Epoch 43/50\n",
      "9984/9984 [==============================] - 16s 2ms/step - loss: 0.0409 - accuracy: 0.9876 - val_loss: 0.6310 - val_accuracy: 0.8277\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.44930\n",
      "Epoch 44/50\n",
      "9984/9984 [==============================] - 16s 2ms/step - loss: 0.0419 - accuracy: 0.9872 - val_loss: 0.6605 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.44930\n",
      "Epoch 45/50\n",
      "9984/9984 [==============================] - 18s 2ms/step - loss: 0.0390 - accuracy: 0.9879 - val_loss: 0.6715 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.44930\n",
      "Epoch 46/50\n",
      "9984/9984 [==============================] - 20s 2ms/step - loss: 0.0369 - accuracy: 0.9887 - val_loss: 0.6710 - val_accuracy: 0.8261\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.44930\n",
      "Epoch 47/50\n",
      "9984/9984 [==============================] - 21s 2ms/step - loss: 0.0330 - accuracy: 0.9898 - val_loss: 0.6706 - val_accuracy: 0.8273\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.44930\n",
      "Epoch 48/50\n",
      "9984/9984 [==============================] - 18s 2ms/step - loss: 0.0350 - accuracy: 0.9907 - val_loss: 0.6772 - val_accuracy: 0.8249\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.44930\n",
      "Epoch 49/50\n",
      "9984/9984 [==============================] - 16s 2ms/step - loss: 0.0377 - accuracy: 0.9874 - val_loss: 0.6844 - val_accuracy: 0.8277\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.44930\n",
      "Epoch 50/50\n",
      "9984/9984 [==============================] - 17s 2ms/step - loss: 0.0339 - accuracy: 0.9896 - val_loss: 0.6958 - val_accuracy: 0.8301\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.44930\n"
     ]
    }
   ],
   "source": [
    "train_top_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The composite model is built by retaining the VGG19 (w/o top layers) model as the base and the newly trained top model for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model layers #:  22\n",
      "Composite Model layers #:  23\n"
     ]
    }
   ],
   "source": [
    "from keras import Model\n",
    "base_model =  applications.VGG19(include_top=False, \n",
    "                                      weights='imagenet',\n",
    "                                      input_shape=(IMG_H, IMG_W, IMG_D))\n",
    "\n",
    "print (\"Base Model layers #: \", len(base_model.layers))\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "composite_model = Model(inputs= base_model.input, outputs= top_model(base_model.output))\n",
    "\n",
    "print (\"Composite Model layers #: \", len(composite_model.layers))\n",
    "\n",
    "#composite_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us test the composite model against the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 81.0400%\n"
     ]
    }
   ],
   "source": [
    "# evaluate and print the test accuracy\n",
    "# get index of predicted smile detection for each image in test set\n",
    "smile_prediction = [np.argmax(composite_model.predict(np.expand_dims(test_data, axis=0))) for test_data in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(smile_prediction)==np.argmax(test_targets, axis=1))/len(smile_prediction)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here we see that an accuracy of 81.04% is achieved considering only the composite model and weights considering the subset of the CelibA dataset. Now, we unfreeze the last Convolution block in VGG-19. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0:input_5 Trainable: False\n",
      "Layer: 1:block1_conv1 Trainable: False\n",
      "Layer: 2:block1_conv2 Trainable: False\n",
      "Layer: 3:block1_pool Trainable: False\n",
      "Layer: 4:block2_conv1 Trainable: False\n",
      "Layer: 5:block2_conv2 Trainable: False\n",
      "Layer: 6:block2_pool Trainable: False\n",
      "Layer: 7:block3_conv1 Trainable: False\n",
      "Layer: 8:block3_conv2 Trainable: False\n",
      "Layer: 9:block3_conv3 Trainable: False\n",
      "Layer: 10:block3_conv4 Trainable: False\n",
      "Layer: 11:block3_pool Trainable: False\n",
      "Layer: 12:block4_conv1 Trainable: False\n",
      "Layer: 13:block4_conv2 Trainable: False\n",
      "Layer: 14:block4_conv3 Trainable: False\n",
      "Layer: 15:block4_conv4 Trainable: False\n",
      "Layer: 16:block4_pool Trainable: False\n",
      "Layer: 17:block5_conv1 Trainable: True\n",
      "Layer: 18:block5_conv2 Trainable: True\n",
      "Layer: 19:block5_conv3 Trainable: True\n",
      "Layer: 20:block5_conv4 Trainable: True\n",
      "Layer: 21:block5_pool Trainable: True\n",
      "Layer: 22:sequential_25 Trainable: True\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "#Freezing all layers except the 5th convolution block in the VGG19 model.\n",
    "\n",
    "for layer in composite_model.layers[:17]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the composite model with SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "composite_model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 218, 178, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 218, 178, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 218, 178, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 109, 89, 64)       0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 109, 89, 128)      73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 109, 89, 128)      147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 54, 44, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 54, 44, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 54, 44, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 54, 44, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 54, 44, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 27, 22, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 27, 22, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 27, 22, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 27, 22, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 27, 22, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 13, 11, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 13, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 13, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 13, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 13, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 6, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_25 (Sequential)   (None, 2)                 3932930   \n",
      "=================================================================\n",
      "Total params: 23,957,314\n",
      "Trainable params: 13,372,162\n",
      "Non-trainable params: 10,585,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "composite_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shall try improving upon this by making use of Data Augmentation to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_AUGMENTATION = True\n",
    "\n",
    "if (not USE_AUGMENTATION):\n",
    "    savedFileName = 'input/saved_models/transfer_models.weights.best.hdf5'\n",
    "else:\n",
    "    savedFileName = 'input/saved_models/transfer_models_btnk_aug.weights.best.hdf5'\n",
    "    \n",
    "checkpointer = ModelCheckpoint(filepath=savedFileName,\n",
    "                               verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n",
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_input,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip = True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'input/dataset/train',\n",
    "        target_size=(IMG_H,IMG_W),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical')\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_input)\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "        'input/dataset/validate',\n",
    "        target_size=(IMG_H,IMG_W),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "312/312 [==============================] - 5775s 19s/step - loss: 0.4446 - accuracy: 0.8001 - val_loss: 0.0631 - val_accuracy: 0.8868\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.06307, saving model to input/saved_models/transfer_models_btnk_aug.weights.best.hdf5\n",
      "Epoch 2/5\n",
      "312/312 [==============================] - 7223s 23s/step - loss: 0.3090 - accuracy: 0.8683 - val_loss: 0.1707 - val_accuracy: 0.9096\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.06307\n",
      "Epoch 3/5\n",
      "312/312 [==============================] - 5948s 19s/step - loss: 0.2747 - accuracy: 0.8854 - val_loss: 0.0822 - val_accuracy: 0.8852\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.06307\n",
      "Epoch 4/5\n",
      "312/312 [==============================] - 5782s 19s/step - loss: 0.2524 - accuracy: 0.8982 - val_loss: 0.0049 - val_accuracy: 0.9100\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.06307 to 0.00493, saving model to input/saved_models/transfer_models_btnk_aug.weights.best.hdf5\n",
      "Epoch 5/5\n",
      "312/312 [==============================] - 5833s 19s/step - loss: 0.2450 - accuracy: 0.9004 - val_loss: 0.0130 - val_accuracy: 0.9080\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00493\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "csv_logger = CSVLogger('logs/composite_aug_finetune.log', append = True)\n",
    "\n",
    "if (not USE_AUGMENTATION):    \n",
    "    composite_model.fit(train_tensors, \n",
    "                 train_targets, \n",
    "                 validation_data=(valid_tensors, alid_targets),\n",
    "                 epochs=NUM_EPOCHS, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 callbacks=[checkpointer], \n",
    "                 verbose=1)\n",
    "else:\n",
    "    composite_model.fit_generator(train_generator,\n",
    "                          validation_data=val_generator,\n",
    "                          steps_per_epoch = NUM_IMAGES // BATCH_SIZE,\n",
    "                          epochs=NUM_EPOCHS, \n",
    "                          callbacks=[checkpointer, csv_logger], \n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with the best validation accuracy\n",
    "composite_model.load_weights(savedFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 90.6400%\n"
     ]
    }
   ],
   "source": [
    "# evaluate and print the test accuracy\n",
    "# get index of predicted smile detection for each image in test set\n",
    "smile_prediction = [np.argmax(composite_model.predict(np.expand_dims(test_data, axis=0))) for test_data in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(smile_prediction)==np.argmax(test_targets, axis=1))/len(smile_prediction)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here, we have the following:\n",
    "1. Store bottleneck features from VGG-19 model\n",
    "2. Train the top-level model using bottle neck features. \n",
    "3. Un-freeze the final convolution layer in VGG-19\n",
    "4. Retrain - the last convolution layer + smile-top-layer with starting weights from previous run.\n",
    "\n",
    "With the above steps also considering Data Augmentation, an accuracy of 90.64% is achieved.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
