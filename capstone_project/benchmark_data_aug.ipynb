{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "\n",
    "## Capstone Project - Smile Detector\n",
    "\n",
    "\n",
    "---\n",
    "In this project, CNNs are used to build models to detect if the person in the image is smiling or not.\n",
    "CelibA dataset is used for this purpose - http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
    "The data for this project is downloaded from Kaggle - https://www.kaggle.com/jessicali9530/celeba-dataset\n",
    "\n",
    "In this notebook, an attempt is successfully made to improve the proposed benchmark model. This is achieved here by using the method of Data Augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_H=218\n",
    "IMG_W=178\n",
    "IMG_D=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, we populate a few variables through the use of the load_files function from the scikit-learn library:\n",
    "\n",
    "train_files, valid_files, test_files - numpy arrays containing file paths to images train_targets, valid_targets, test_targets - numpy arrays containing onehot-encoded classification labels smile_names - list of string-valued smile categories for translating labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 total smile categories.\n",
      "There are 15000 total smile images.\n",
      "\n",
      "There are 10000 training smile images.\n",
      "There are 2500 validation smile images.\n",
      "There are 2500 test smile images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path, load_content = False)\n",
    "    smile_files = np.array(data['filenames'])\n",
    "    smile_targets = np_utils.to_categorical(np.array(data['target']), 2)\n",
    "    return smile_files, smile_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('input/dataset/train')\n",
    "valid_files, valid_targets = load_dataset('input/dataset/validate')\n",
    "test_files, test_targets = load_dataset('input/dataset/test')\n",
    "\n",
    "smile_names = [item[:-1] for item in sorted(glob(\"input/dataset/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total smile categories.' % len(smile_names))\n",
    "print('There are %s total smile images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training smile images.' % len(train_files))\n",
    "print('There are %d validation smile images.' % len(valid_files))\n",
    "print('There are %d test smile images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using TensorFlow as backend, Keras CNNs require a 4D array as input. Below is the function for the same. Also, preprocess input is used from imagenet_utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(IMG_H, IMG_W))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (IMG_H, IMG_W, IMG_D)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, IMG_H, IMG_W, IMG_D) and return 4D tensor\n",
    "    x = imagenet_utils.preprocess_input(x)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "    \n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2500 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 56/2500 [00:00<00:04, 558.72it/s]\u001b[A\n",
      "  5%|▍         | 117/2500 [00:00<00:04, 573.17it/s]\u001b[A\n",
      "  7%|▋         | 168/2500 [00:00<00:04, 552.61it/s]\u001b[A\n",
      "  9%|▉         | 227/2500 [00:00<00:04, 558.48it/s]\u001b[A\n",
      " 11%|█▏        | 287/2500 [00:00<00:03, 568.70it/s]\u001b[A\n",
      " 14%|█▎        | 343/2500 [00:00<00:03, 564.43it/s]\u001b[A\n",
      " 16%|█▌        | 397/2500 [00:00<00:03, 556.77it/s]\u001b[A\n",
      " 19%|█▊        | 463/2500 [00:00<00:03, 581.18it/s]\u001b[A\n",
      " 21%|██        | 521/2500 [00:00<00:03, 580.79it/s]\u001b[A\n",
      " 23%|██▎       | 584/2500 [00:01<00:03, 593.08it/s]\u001b[A\n",
      " 26%|██▌       | 642/2500 [00:01<00:03, 580.17it/s]\u001b[A\n",
      " 28%|██▊       | 700/2500 [00:01<00:03, 556.13it/s]\u001b[A\n",
      " 30%|███       | 762/2500 [00:01<00:03, 573.48it/s]\u001b[A\n",
      " 33%|███▎      | 820/2500 [00:01<00:02, 568.18it/s]\u001b[A\n",
      " 35%|███▌      | 885/2500 [00:01<00:02, 590.04it/s]\u001b[A\n",
      " 38%|███▊      | 945/2500 [00:01<00:02, 575.36it/s]\u001b[A\n",
      " 40%|████      | 1009/2500 [00:01<00:02, 592.89it/s]\u001b[A\n",
      " 43%|████▎     | 1069/2500 [00:01<00:02, 564.89it/s]\u001b[A\n",
      " 45%|████▌     | 1132/2500 [00:01<00:02, 580.00it/s]\u001b[A\n",
      " 48%|████▊     | 1191/2500 [00:02<00:02, 579.07it/s]\u001b[A\n",
      " 50%|█████     | 1250/2500 [00:02<00:02, 559.06it/s]\u001b[A\n",
      " 53%|█████▎    | 1314/2500 [00:02<00:02, 579.74it/s]\u001b[A\n",
      " 55%|█████▌    | 1376/2500 [00:02<00:01, 589.59it/s]\u001b[A\n",
      " 58%|█████▊    | 1438/2500 [00:02<00:01, 593.34it/s]\u001b[A\n",
      " 60%|█████▉    | 1498/2500 [00:02<00:01, 586.14it/s]\u001b[A\n",
      " 62%|██████▏   | 1557/2500 [00:02<00:01, 574.68it/s]\u001b[A\n",
      " 65%|██████▍   | 1618/2500 [00:02<00:01, 584.38it/s]\u001b[A\n",
      " 67%|██████▋   | 1679/2500 [00:02<00:01, 591.41it/s]\u001b[A\n",
      " 70%|██████▉   | 1739/2500 [00:03<00:01, 578.00it/s]\u001b[A\n",
      " 72%|███████▏  | 1797/2500 [00:03<00:01, 536.31it/s]\u001b[A\n",
      " 74%|███████▍  | 1852/2500 [00:03<00:01, 535.16it/s]\u001b[A\n",
      " 76%|███████▋  | 1910/2500 [00:03<00:01, 547.43it/s]\u001b[A\n",
      " 79%|███████▉  | 1969/2500 [00:03<00:00, 559.11it/s]\u001b[A\n",
      " 81%|████████  | 2026/2500 [00:03<00:00, 556.87it/s]\u001b[A\n",
      " 83%|████████▎ | 2082/2500 [00:03<00:00, 544.36it/s]\u001b[A\n",
      " 85%|████████▌ | 2137/2500 [00:03<00:00, 515.33it/s]\u001b[A\n",
      " 88%|████████▊ | 2190/2500 [00:03<00:00, 505.81it/s]\u001b[A\n",
      " 90%|████████▉ | 2241/2500 [00:03<00:00, 488.31it/s]\u001b[A\n",
      " 92%|█████████▏| 2291/2500 [00:04<00:00, 478.63it/s]\u001b[A\n",
      " 94%|█████████▎| 2340/2500 [00:04<00:00, 452.19it/s]\u001b[A\n",
      " 95%|█████████▌| 2386/2500 [00:04<00:00, 431.53it/s]\u001b[A\n",
      " 97%|█████████▋| 2430/2500 [00:04<00:00, 421.44it/s]\u001b[A\n",
      "100%|██████████| 2500/2500 [00:04<00:00, 544.06it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "#train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "#valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BenchModel Architecture\n",
    "\n",
    "Below is our Benchmark model - a simple CNN model created with few Conv2D, MaxPooling and Drop out layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 218, 178, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 109, 89, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 109, 89, 32)       2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 54, 44, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 54, 44, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 27, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 27, 22, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 43,698\n",
      "Trainable params: 43,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=16,kernel_size=2,padding='same',activation='relu',input_shape=(IMG_H,IMG_W,IMG_D)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32,kernel_size=2,padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64,kernel_size=2,padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters=128,kernel_size=2,padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "We will now augment the data via a number of random transformations so that our model would never see the exact same picture. This helps prevent overfitting and helps the model generalize better. Robustness of the model increases with this augmentation technique. The image generators of augmented image batches (and their lables) are instantiated either via .flow(data, labels) or .flow_from_directory(directory). In this notebook, .flow_from_directory(directory) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n",
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = imagenet_utils.preprocess_input,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'input/dataset/train',\n",
    "        target_size=(IMG_H,IMG_W),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "val_datagen = ImageDataGenerator(preprocessing_function = imagenet_utils.preprocess_input)\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "        'input/dataset/validate',\n",
    "        target_size=(IMG_H,IMG_W),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "312/312 [==============================] - 425s 1s/step - loss: 1.3111 - accuracy: 0.5246 - val_loss: 0.6977 - val_accuracy: 0.6072\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69769, saving model to input/saved_models/rmsprop_benchmark_aug.weights.best.hdf5\n",
      "Epoch 2/20\n",
      "312/312 [==============================] - 403s 1s/step - loss: 0.6740 - accuracy: 0.5880 - val_loss: 0.4702 - val_accuracy: 0.5796\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69769 to 0.47019, saving model to input/saved_models/rmsprop_benchmark_aug.weights.best.hdf5\n",
      "Epoch 3/20\n",
      "312/312 [==============================] - 399s 1s/step - loss: 0.6362 - accuracy: 0.6473 - val_loss: 0.4374 - val_accuracy: 0.7380\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.47019 to 0.43744, saving model to input/saved_models/rmsprop_benchmark_aug.weights.best.hdf5\n",
      "Epoch 4/20\n",
      "312/312 [==============================] - 388s 1s/step - loss: 0.5774 - accuracy: 0.7093 - val_loss: 0.4459 - val_accuracy: 0.7652\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.43744\n",
      "Epoch 5/20\n",
      "312/312 [==============================] - 385s 1s/step - loss: 0.5240 - accuracy: 0.7461 - val_loss: 0.9835 - val_accuracy: 0.6520\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.43744\n",
      "Epoch 6/20\n",
      "312/312 [==============================] - 388s 1s/step - loss: 0.4974 - accuracy: 0.7659 - val_loss: 0.4036 - val_accuracy: 0.8024\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.43744 to 0.40361, saving model to input/saved_models/rmsprop_benchmark_aug.weights.best.hdf5\n",
      "Epoch 7/20\n",
      "312/312 [==============================] - 365s 1s/step - loss: 0.4702 - accuracy: 0.7808 - val_loss: 0.2224 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.40361 to 0.22242, saving model to input/saved_models/rmsprop_benchmark_aug.weights.best.hdf5\n",
      "Epoch 8/20\n",
      "312/312 [==============================] - 369s 1s/step - loss: 0.4465 - accuracy: 0.7980 - val_loss: 0.0877 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.22242 to 0.08769, saving model to input/saved_models/rmsprop_benchmark_aug.weights.best.hdf5\n",
      "Epoch 9/20\n",
      "312/312 [==============================] - 358s 1s/step - loss: 0.4249 - accuracy: 0.8088 - val_loss: 0.1411 - val_accuracy: 0.7620\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.08769\n",
      "Epoch 10/20\n",
      "312/312 [==============================] - 363s 1s/step - loss: 0.4143 - accuracy: 0.8134 - val_loss: 0.3439 - val_accuracy: 0.8204\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.08769\n",
      "Epoch 11/20\n",
      "312/312 [==============================] - 367s 1s/step - loss: 0.3966 - accuracy: 0.8245 - val_loss: 0.1819 - val_accuracy: 0.8332\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.08769\n",
      "Epoch 12/20\n",
      "312/312 [==============================] - 367s 1s/step - loss: 0.3919 - accuracy: 0.8301 - val_loss: 0.2693 - val_accuracy: 0.8456\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.08769\n",
      "Epoch 13/20\n",
      "312/312 [==============================] - 359s 1s/step - loss: 0.3757 - accuracy: 0.8378 - val_loss: 0.1953 - val_accuracy: 0.8432\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.08769\n",
      "Epoch 14/20\n",
      "312/312 [==============================] - 357s 1s/step - loss: 0.3680 - accuracy: 0.8394 - val_loss: 0.2130 - val_accuracy: 0.8488\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.08769\n",
      "Epoch 15/20\n",
      "312/312 [==============================] - 357s 1s/step - loss: 0.3615 - accuracy: 0.8437 - val_loss: 0.3422 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.08769\n",
      "Epoch 16/20\n",
      "312/312 [==============================] - 360s 1s/step - loss: 0.3578 - accuracy: 0.8444 - val_loss: 0.5454 - val_accuracy: 0.8556\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.08769\n",
      "Epoch 17/20\n",
      "312/312 [==============================] - 374s 1s/step - loss: 0.3568 - accuracy: 0.8422 - val_loss: 0.2703 - val_accuracy: 0.8572\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.08769\n",
      "Epoch 18/20\n",
      "312/312 [==============================] - 358s 1s/step - loss: 0.3446 - accuracy: 0.8510 - val_loss: 1.9181 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.08769\n",
      "Epoch 19/20\n",
      "312/312 [==============================] - 352s 1s/step - loss: 0.3393 - accuracy: 0.8523 - val_loss: 0.1770 - val_accuracy: 0.8660\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.08769\n",
      "Epoch 20/20\n",
      "312/312 [==============================] - 352s 1s/step - loss: 0.3353 - accuracy: 0.8573 - val_loss: 0.2283 - val_accuracy: 0.8552\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.08769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x286c3adda08>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "model_weights_filepath='input/saved_models/rmsprop_benchmark_aug.weights.best.hdf5'\n",
    "checkpointer = ModelCheckpoint(model_weights_filepath,verbose=1,save_best_only=True)\n",
    "csvLogger = CSVLogger('logs/rmsprop_benchmark_aug',append = True)\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=10000//32, # No.Of Training Samples/Batch_size\n",
    "        epochs=20,\n",
    "        validation_data=val_generator,\n",
    "        callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with the best validation accuracy\n",
    "model.load_weights(model_weights_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 82.1200%\n"
     ]
    }
   ],
   "source": [
    "# evaluate and print the test accuracy\n",
    "# get index of predicted smile detection for each image in test set\n",
    "smile_prediction = [np.argmax(model.predict(np.expand_dims(test_data, axis=0))) for test_data in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(smile_prediction)==np.argmax(test_targets, axis=1))/len(smile_prediction)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here, we saw that the benchmark model when considered with augmentation of data gave a test accuracy of 82.10%. This is slightly higher than the accuracy of the benchmark model considered."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
